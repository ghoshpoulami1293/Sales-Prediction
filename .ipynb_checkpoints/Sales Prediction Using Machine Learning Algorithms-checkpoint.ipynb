{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99627ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\ghosh\\anaconda_edx\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\ghosh\\anaconda_edx\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ghosh\\anaconda_edx\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ghosh\\anaconda_edx\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\ghosh\\anaconda_edx\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab7e11cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\ghosh\\anaconda_edx\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\ghosh\\anaconda_edx\\lib\\site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\ghosh\\anaconda_edx\\lib\\site-packages (from xgboost) (1.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a4eebc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R-squared (R2) Score: 1.0\n",
      "Mean Squared Error (MSE): 4.541694326203986e-17\n",
      "Mean Absolute Error (MAE): 5.846804818077089e-09\n",
      "R-squared (R2) Score: 1.0\n",
      "************************************************************\n",
      "Random Forest R-squared (R2) Score: 0.9999154708845687\n",
      "Mean Squared Error (MSE): 5.573432897271934\n",
      "Mean Absolute Error (MAE): 1.15694215000003\n",
      "R-squared (R2) Score: 0.9999154708845687\n",
      "************************************************************\n",
      "Gradient Boosting R-squared (R2) Score: 0.9999383758639572\n",
      "Mean Squared Error (MSE): 4.063191544527292\n",
      "Mean Absolute Error (MAE): 1.4445446730840246\n",
      "R-squared (R2) Score: 0.9999383758639572\n",
      "************************************************************\n",
      "Total Income for Linear Regression:\n",
      "    permonth  Predicted Total  Actual Total\n",
      "521  1970-01         523.9710      523.9710\n",
      "737  1970-01         616.9800      616.9800\n",
      "740  1970-01         408.7335      408.7335\n",
      "660  1970-01         135.3555      135.3555\n",
      "411  1970-01          45.9270       45.9270\n",
      "..       ...              ...           ...\n",
      "468  1970-01          22.6590       22.6590\n",
      "935  1970-01         270.0180      270.0180\n",
      "428  1970-01         353.1675      353.1675\n",
      "7    1970-01         772.3800      772.3800\n",
      "155  1970-01         484.5225      484.5225\n",
      "\n",
      "[300 rows x 3 columns]\n",
      "Total Income for Random Forest:\n",
      "    permonth  Predicted Total  Actual Total\n",
      "521  1970-01       522.702705      523.9710\n",
      "737  1970-01       621.489540      616.9800\n",
      "740  1970-01       408.332925      408.7335\n",
      "660  1970-01       135.673755      135.3555\n",
      "411  1970-01        44.613765       45.9270\n",
      "..       ...              ...           ...\n",
      "468  1970-01        23.163945       22.6590\n",
      "935  1970-01       271.218255      270.0180\n",
      "428  1970-01       352.680825      353.1675\n",
      "7    1970-01       769.188315      772.3800\n",
      "155  1970-01       485.598645      484.5225\n",
      "\n",
      "[300 rows x 3 columns]\n",
      "Total Income for Gradient Boosting:\n",
      "    permonth  Predicted Total  Actual Total\n",
      "521  1970-01       523.645222      523.9710\n",
      "737  1970-01       619.398826      616.9800\n",
      "740  1970-01       410.916548      408.7335\n",
      "660  1970-01       134.588602      135.3555\n",
      "411  1970-01        44.845755       45.9270\n",
      "..       ...              ...           ...\n",
      "468  1970-01        22.471044       22.6590\n",
      "935  1970-01       271.064366      270.0180\n",
      "428  1970-01       352.852555      353.1675\n",
      "7    1970-01       768.812774      772.3800\n",
      "155  1970-01       485.001654      484.5225\n",
      "\n",
      "[300 rows x 3 columns]\n",
      "Best Model: Linear Regression, R-squared (R2) Score: 1.0\n",
      "Feature importance is not available for the selected model or the model does not support it.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Read the csv for further processing\n",
    "def read_dataset(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Data preprocessing \n",
    "def preprocess_data(df):\n",
    "    # Drop rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Convert 'Date' column to datetime type\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Convert datetime to numeric representation\n",
    "    df['Date'] = df['Date'].astype(np.int64) // 10**9 \n",
    "    \n",
    "    # Drop irrelevant columns if they exist\n",
    "    irrelevant_columns = ['Customer ID', 'Invoice ID', 'Time', 'cogs', 'gross margin percentage', 'gross income', 'Rating']\n",
    "    df = df.drop(columns=[col for col in irrelevant_columns if col in df.columns], errors='ignore')\n",
    "\n",
    "    # Encode categorical variables\n",
    "    le = LabelEncoder()\n",
    "    df['Gender'] = le.fit_transform(df['Gender'])\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    df = pd.get_dummies(df, columns=['City', 'Customer type', 'Branch', 'Product line', 'Payment'], drop_first=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Split data into features and target\n",
    "def split_data(df):\n",
    "    X = df.drop('Total', axis=1)\n",
    "    y = df['Total']\n",
    "    return train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "def hyperparameter_tuning(model, params, X_train, y_train):\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=params, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Print metrics for each model\n",
    "def print_metrics(y_test, y_pred):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"R-squared (R2) Score:\", r2)\n",
    "    print(\"*\" * 60)\n",
    "\n",
    "# Plotting feature importance if the model supports it \n",
    "def plot_feature_importance(features, importances, best_model):    \n",
    "    if importances is None:\n",
    "        print(\"Feature importances not available for the selected model.\")\n",
    "        return\n",
    "    feature_importance = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title(f'Feature Importance for {best_model}')\n",
    "    plt.show()\n",
    "    print(\"Feature importance plot generated successfully!\")\n",
    "\n",
    "#Executing the program\n",
    "if __name__ == \"__main__\":\n",
    "    # Reading the dataset\n",
    "    df = read_dataset(\"supermarket_sales.csv\")\n",
    "    \n",
    "    # Preprocessing the data\n",
    "    df = preprocess_data(df)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = split_data(df)\n",
    "\n",
    "    # Defining the models being used (chosen based on the type of data - continuous)\n",
    "    mlModels = {        \n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"Random Forest\": RandomForestRegressor(),\n",
    "        \"Gradient Boosting\": GradientBoostingRegressor()\n",
    "    }\n",
    "\n",
    "   # Hyperparameter Tuning and best model choosing\n",
    "    best_r2_score = -float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for name, model in mlModels.items():\n",
    "        params = {}  # Defining the hyperparameters for each model\n",
    "        mlModels[name] = hyperparameter_tuning(model, params, X_train, y_train)\n",
    "        y_pred = mlModels[name].predict(X_test)\n",
    "        r2_score_current = r2_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"{name} R-squared (R2) Score:\", r2_score_current)\n",
    "        \n",
    "        if r2_score_current > best_r2_score:\n",
    "            best_r2_score = r2_score_current\n",
    "            best_model = name\n",
    "        print_metrics(y_test, y_pred) # Printing the metrics for each model\n",
    "\n",
    "    # Predicted total  for each model vs the actual total in the dataset for comparison\n",
    "    for name, model in mlModels.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Aggregate predicted and actual sales per month\n",
    "            df_test = X_test.copy()\n",
    "            df_test[\"Predicted Total\"] = y_pred\n",
    "            df_test[\"Actual Total\"] = df[\"Total\"]\n",
    "\n",
    "            df_test[\"permonth\"] = pd.to_datetime(df_test[\"Date\"]).dt.to_period(\"M\")\n",
    "            print(f\"Total Income for {name}:\")\n",
    "            print(df_test[[\"permonth\", \"Predicted Total\", \"Actual Total\"]])\n",
    "            \n",
    "\n",
    "    # Print the best model and its R2 score\n",
    "    print(f\"Best Model: {best_model}, R-squared (R2) Score: {best_r2_score}\")\n",
    "    \n",
    "    # Feature importance graph \n",
    "    best_clf = mlModels[best_model]\n",
    "    if best_model in ['Random Forest', 'Gradient Boosting'] and hasattr(best_clf, 'feature_importances_'):\n",
    "        feature_importance = best_clf.feature_importances_\n",
    "        features = X_train.columns\n",
    "        plot_feature_importance(features, feature_importance, best_model)\n",
    "    else:\n",
    "        print(\"Feature importance is not available for the selected model or the model does not support it.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
